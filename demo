#!/usr/bin/env python3
"""Helper script to launch a full demo of the low-latency stream kit."""
from __future__ import annotations

import argparse
import asyncio
import importlib.metadata
import shlex
import os
import shutil
import signal
import subprocess
import sys
import tempfile
import threading
import time
from datetime import UTC, datetime
from pathlib import Path
from textwrap import dedent
from typing import Any, Dict, Iterable, List, Sequence, Tuple

from tspi_kit.commands import COMMAND_SUBJECT_PREFIX
from tspi_kit.datastore import MessageRecord, TagRecord

PROJECT_ROOT = Path(__file__).resolve().parent
APT_UPDATED = False


def parse_args(argv: List[str] | None = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Run the HA JetStream demo setup.")
    parser.add_argument(
        "--duration",
        type=float,
        default=None,
        help="Optional duration (seconds) to run before shutting down. Defaults to running until interrupted.",
    )
    parser.add_argument(
        "--count",
        type=int,
        default=10,
        help="Number of simulated aircraft to generate.",
    )
    parser.add_argument(
        "--rate",
        type=float,
        default=20.0,
        help="Generation rate in Hertz for the simulator.",
    )
    parser.add_argument(
        "--log-dir",
        type=Path,
        default=None,
        help="Optional directory to store JetStream node logs. Defaults to a temporary directory.",
    )
    return parser.parse_args(argv)


def ensure_python_version() -> None:
    if sys.version_info < (3, 11):
        raise RuntimeError("Python 3.11 or newer is required to run the demo helper script.")


def ensure_command(command: str, *, apt_package: str | None = None) -> None:
    if shutil.which(command):
        return
    if apt_package and shutil.which("apt-get"):
        install_with_apt(apt_package)
        if shutil.which(command):
            return
    raise RuntimeError(f"Required command '{command}' is not available on PATH.")


def install_with_apt(package: str) -> None:
    global APT_UPDATED
    env = os.environ.copy()
    env.setdefault("DEBIAN_FRONTEND", "noninteractive")
    if not APT_UPDATED:
        subprocess.run(["apt-get", "update"], check=True, env=env, stdout=subprocess.DEVNULL)
        APT_UPDATED = True
    subprocess.run(["apt-get", "install", "-y", package], check=True, env=env)


def read_requirements(path: Path) -> List[Tuple[str, str]]:
    requirements: List[Tuple[str, str]] = []
    for raw_line in path.read_text().splitlines():
        line = raw_line.strip()
        if not line or line.startswith("#"):
            continue
        if raw_line.startswith(" ") or raw_line.startswith("\t"):
            continue
        requirement = line.split("#", 1)[0].strip()
        if not requirement:
            continue
        name = requirement
        for delimiter in ("==", ">=", "<=", "~=", ">", "<"):
            if delimiter in requirement:
                name = requirement.split(delimiter, 1)[0].strip()
                break
        requirements.append((name, requirement))
    return requirements


def ensure_python_dependencies(requirements_path: Path) -> None:
    missing: List[str] = []
    for name, requirement in read_requirements(requirements_path):
        try:
            installed_version = importlib.metadata.version(name)
        except importlib.metadata.PackageNotFoundError:
            missing.append(requirement)
            continue
        if "==" in requirement:
            expected = requirement.split("==", 1)[1].strip()
            if installed_version != expected:
                missing.append(requirement)
    if missing:
        print(f"Installing missing Python packages: {', '.join(missing)}")
        subprocess.check_call([sys.executable, "-m", "pip", "install", *missing])
    else:
        print("All required Python packages are already installed.")


class JetStreamClusterManager:
    """Manage a local multi-node JetStream cluster."""

    def __init__(
        self,
        replicas: int = 3,
        *,
        base_client_port: int = 4222,
        base_cluster_port: int = 6222,
        base_monitor_port: int = 8222,
        log_dir: Path | None = None,
    ) -> None:
        self.replicas = replicas
        self.base_client_port = base_client_port
        self.base_cluster_port = base_cluster_port
        self.base_monitor_port = base_monitor_port
        self._temp_dir_obj = tempfile.TemporaryDirectory() if log_dir is None else None
        self.base_dir = (Path(self._temp_dir_obj.name) if self._temp_dir_obj else log_dir).resolve()
        self.config_dir = self.base_dir / "configs"
        self.store_dir = self.base_dir / "store"
        self.logs_dir = self.base_dir / "logs"
        self.config_dir.mkdir(parents=True, exist_ok=True)
        self.store_dir.mkdir(parents=True, exist_ok=True)
        self.logs_dir.mkdir(parents=True, exist_ok=True)
        self._processes: List[subprocess.Popen[str]] = []
        self._log_handles: List[object] = []

    @property
    def client_urls(self) -> List[str]:
        return [f"nats://127.0.0.1:{self.base_client_port + index}" for index in range(self.replicas)]

    def _config_text(self, index: int) -> str:
        node_name = f"demo-node-{index + 1}"
        client_port = self.base_client_port + index
        cluster_port = self.base_cluster_port + index
        monitor_port = self.base_monitor_port + index
        store_path = (self.store_dir / node_name).as_posix()
        routes = [
            f'        "nats://127.0.0.1:{self.base_cluster_port + route}"'
            for route in range(self.replicas)
            if route != index
        ]
        routes_block = "\n".join(routes)
        if routes_block:
            routes_block = "\n" + routes_block + "\n    "
        return dedent(
            f"""
            server_name: "{node_name}"
            port: {client_port}
            http: {monitor_port}
            jetstream: {{
              store_dir: "{store_path}"
            }}
            cluster {{
              name: "demo"
              listen: "127.0.0.1:{cluster_port}"
              routes = [{routes_block}]
            }}
            """
        ).strip()

    def start(self) -> None:
        for index in range(self.replicas):
            config_path = self.config_dir / f"node_{index + 1}.conf"
            config_path.write_text(self._config_text(index))
            log_path = self.logs_dir / f"node_{index + 1}.log"
            log_handle = log_path.open("w")
            process = subprocess.Popen(
                ["nats-server", "-c", str(config_path)],
                stdout=log_handle,
                stderr=subprocess.STDOUT,
                text=True,
            )
            self._processes.append(process)
            self._log_handles.append(log_handle)
        print(f"Started JetStream cluster with {self.replicas} nodes. Logs at {self.logs_dir}")

    def stop(self) -> None:
        for process in self._processes:
            if process.poll() is None:
                process.terminate()
        for process in self._processes:
            if process.poll() is None:
                try:
                    process.wait(timeout=5)
                except subprocess.TimeoutExpired:
                    process.kill()
        for handle in self._log_handles:
            try:
                handle.close()
            except Exception:
                pass
        self._processes.clear()
        self._log_handles.clear()
        if self._temp_dir_obj is not None:
            self._temp_dir_obj.cleanup()


class InMemoryTimescaleReplica:
    """Simple in-memory backing store used to simulate a Timescale node."""

    def __init__(self, name: str) -> None:
        self.name = name
        self._messages: List[MessageRecord] = []
        self._commands: Dict[str, Dict[str, Any]] = {}
        self._tags: Dict[str, TagRecord] = {}
        self._lock = asyncio.Lock()

    async def store_message(self, record: MessageRecord) -> None:
        async with self._lock:
            self._messages.append(record)

    async def store_command(self, record: Dict[str, Any]) -> None:
        cmd_id = record.get("cmd_id")
        if not cmd_id:
            return
        async with self._lock:
            self._commands[str(cmd_id)] = dict(record)

    async def store_tag(self, record: TagRecord) -> None:
        async with self._lock:
            self._tags[record.id] = record

    async def count_messages(self) -> int:
        async with self._lock:
            return len(self._messages)

    async def count_commands(self) -> int:
        async with self._lock:
            return len(self._commands)

    async def count_tags(self) -> int:
        async with self._lock:
            return len(self._tags)

    async def close(self) -> None:
        async with self._lock:
            self._messages.clear()
            self._commands.clear()
            self._tags.clear()


class TimescaleHACluster:
    """In-memory HA simulation for the Timescale datastore."""

    def __init__(self, replicas: int = 2) -> None:
        if replicas < 1:
            raise ValueError("At least one datastore replica is required")
        self._replicas = [InMemoryTimescaleReplica(f"ha-node-{index + 1}") for index in range(replicas)]
        self._messages: List[MessageRecord] = []
        self._message_ids: Dict[str, int] = {}
        self._commands: Dict[str, Dict[str, Any]] = {}
        self._tags: Dict[str, TagRecord] = {}
        self._next_id = 1
        self._lock = asyncio.Lock()

    @property
    def replica_count(self) -> int:
        return len(self._replicas)

    def replica_names(self) -> List[str]:
        return [replica.name for replica in self._replicas]

    async def connect(self) -> None:
        return None

    async def close(self) -> None:
        await asyncio.gather(*(replica.close() for replica in self._replicas))
        async with self._lock:
            self._messages.clear()
            self._message_ids.clear()
            self._commands.clear()
            self._tags.clear()

    async def insert_message(
        self,
        *,
        subject: str,
        kind: str,
        payload: Dict[str, Any],
        headers: Dict[str, Any],
        published_ts: datetime,
        raw_cbor: bytes,
    ) -> int | None:
        headers_dict = {str(key): str(value) for key, value in headers.items()}
        payload_dict = dict(payload)
        message_key = headers_dict.get("Nats-Msg-Id")
        async with self._lock:
            if message_key and message_key in self._message_ids:
                return None
            record_id = self._next_id
            self._next_id += 1
            record = MessageRecord(
                id=record_id,
                subject=subject,
                kind=kind,
                published_ts=self._to_timestamp(published_ts),
                headers=headers_dict,
                payload=payload_dict,
                cbor=bytes(raw_cbor),
                recv_epoch_ms=self._coerce_optional_int(payload_dict.get("recv_epoch_ms")),
                recv_iso=self._coerce_optional_iso(payload_dict.get("recv_iso")),
                message_type=self._coerce_optional_str(payload_dict.get("type")),
                sensor_id=self._coerce_optional_int(payload_dict.get("sensor_id")),
                day=self._coerce_optional_int(payload_dict.get("day")),
                time_s=self._coerce_optional_float(payload_dict.get("time_s")),
            )
            self._messages.append(record)
            if message_key:
                self._message_ids[message_key] = record_id
        await asyncio.gather(*(replica.store_message(record) for replica in self._replicas))
        return record_id

    async def fetch_messages_between(self, start_ts: float, end_ts: float) -> Sequence[MessageRecord]:
        async with self._lock:
            return [
                record
                for record in self._messages
                if start_ts <= record.published_ts <= end_ts
            ]

    async def fetch_messages_for_tag(self, tag_id: str, *, window_seconds: float = 10.0) -> Sequence[MessageRecord]:
        tag = await self.get_tag(tag_id)
        if tag is None:
            return []
        centre = self._to_datetime(tag.ts).timestamp()
        half_window = window_seconds / 2.0
        return await self.fetch_messages_between(centre - half_window, centre + half_window)

    async def upsert_command(
        self,
        payload: Dict[str, Any],
        *,
        message_id: int,
        published_ts: datetime,
    ) -> None:
        cmd_id = payload.get("cmd_id")
        if not cmd_id:
            return
        enriched = dict(payload)
        enriched.setdefault("message_id", message_id)
        enriched.setdefault("published_ts", self._to_datetime(published_ts).isoformat())
        key = str(cmd_id)
        async with self._lock:
            self._commands[key] = enriched
        await asyncio.gather(*(replica.store_command(enriched) for replica in self._replicas))

    async def latest_command(self, name: str) -> Dict[str, Any] | None:
        async with self._lock:
            candidates = [
                dict(record)
                for record in self._commands.values()
                if str(record.get("name")) == name
            ]
        if not candidates:
            return None
        return sorted(candidates, key=lambda record: record.get("published_ts", ""), reverse=True)[0]

    async def apply_tag_event(
        self,
        subject: str,
        payload: Dict[str, Any],
        *,
        message_id: int,
    ) -> None:
        tag_id = payload.get("id")
        if not tag_id:
            return
        async with self._lock:
            existing = self._tags.get(str(tag_id))
            base = existing.__dict__ if existing else {}
            ts_value = payload.get("ts", base.get("ts"))
            updated_ts_value = payload.get("ts", base.get("updated_ts"))
            status = payload.get("status")
            if status is None:
                if subject.endswith(".delete"):
                    status = "deleted"
                else:
                    status = base.get("status", "active")
            extra_payload = payload.get("extra", base.get("extra", {}))
            if not isinstance(extra_payload, dict):
                extra_payload = {}
            record = TagRecord(
                id=str(tag_id),
                ts=self._coerce_iso(ts_value),
                creator=self._coerce_optional_str(payload.get("creator", base.get("creator"))),
                label=self._coerce_optional_str(payload.get("label", base.get("label"))),
                category=self._coerce_optional_str(payload.get("category", base.get("category"))),
                notes=self._coerce_optional_str(payload.get("notes", base.get("notes"))),
                extra=dict(extra_payload),
                status=str(status),
                updated_ts=self._coerce_iso(updated_ts_value),
            )
            self._tags[record.id] = record
        await asyncio.gather(*(replica.store_tag(record) for replica in self._replicas))

    async def get_tag(self, tag_id: str) -> TagRecord | None:
        async with self._lock:
            return self._tags.get(tag_id)

    async def count_messages(self) -> int:
        async with self._lock:
            return len(self._messages)

    async def count_commands(self) -> int:
        async with self._lock:
            return len(self._commands)

    async def count_tags(self) -> int:
        async with self._lock:
            return len(self._tags)

    async def replica_message_counts(self) -> List[int]:
        return [count for count in await asyncio.gather(*(replica.count_messages() for replica in self._replicas))]

    async def replica_command_counts(self) -> List[int]:
        return [count for count in await asyncio.gather(*(replica.count_commands() for replica in self._replicas))]

    async def replica_tag_counts(self) -> List[int]:
        return [count for count in await asyncio.gather(*(replica.count_tags() for replica in self._replicas))]

    @staticmethod
    def _to_datetime(value: datetime | float | int | str) -> datetime:
        if isinstance(value, datetime):
            return value.astimezone(UTC) if value.tzinfo else value.replace(tzinfo=UTC)
        if isinstance(value, (float, int)):
            return datetime.fromtimestamp(float(value), tz=UTC)
        return datetime.fromisoformat(str(value)).astimezone(UTC)

    @classmethod
    def _to_timestamp(cls, value: datetime | float | int | str) -> float:
        return cls._to_datetime(value).timestamp()

    @classmethod
    def _coerce_iso(cls, value: object | None) -> str:
        if value is None:
            return cls._to_datetime(datetime.now(tz=UTC)).isoformat()
        if isinstance(value, datetime):
            return cls._to_datetime(value).isoformat()
        return str(value)

    @classmethod
    def _coerce_optional_iso(cls, value: object | None) -> str | None:
        if value is None:
            return None
        return cls._coerce_iso(value)

    @staticmethod
    def _coerce_optional_str(value: object | None) -> str | None:
        if value is None:
            return None
        return str(value)

    @staticmethod
    def _coerce_optional_int(value: object | None) -> int | None:
        if value is None:
            return None
        try:
            return int(value)
        except (TypeError, ValueError):
            return None

    @staticmethod
    def _coerce_optional_float(value: object | None) -> float | None:
        if value is None:
            return None
        try:
            return float(value)
        except (TypeError, ValueError):
            return None

def main(argv: Iterable[str] | None = None) -> int:
    args = parse_args(list(argv) if argv is not None else None)
    ensure_python_version()
    ensure_command("nats-server", apt_package="nats-server")
    requirements_path = PROJECT_ROOT / "requirements.txt"
    if requirements_path.exists():
        ensure_python_dependencies(requirements_path)
    else:
        print("requirements.txt not found; skipping Python dependency check.")

    from nats.aio.client import Client as NATS
    from nats.errors import ErrNoServers
    from nats.js.errors import NotFoundError
    from tspi_kit.archiver import Archiver

    async def connect_to_cluster(servers: List[str]) -> NATS:
        deadline = time.monotonic() + 60.0
        attempt = 0
        while True:
            attempt += 1
            nc = NATS()
            try:
                await nc.connect(
                    servers=servers,
                    connect_timeout=2,
                    max_reconnect_attempts=-1,
                    reconnect_time_wait=0.5,
                )
                print(f"Connected to JetStream cluster on attempt {attempt}.")
                return nc
            except ErrNoServers:
                if time.monotonic() >= deadline:
                    raise RuntimeError("Timed out waiting for JetStream cluster to become ready.")
                await asyncio.sleep(1.0)

    async def prepare_stream(js, replicas: int) -> None:
        stream_name = "TSPI"
        subjects = ["tspi.>", f"{COMMAND_SUBJECT_PREFIX}.>", "tags.>"]
        try:
            await js.stream_info(stream_name)
            await js.update_stream({"name": stream_name, "subjects": subjects, "num_replicas": replicas})
        except NotFoundError:
            await js.add_stream(
                name=stream_name,
                subjects=subjects,
                num_replicas=replicas,
                retention="limits",
                max_msgs=-1,
                max_bytes=-1,
            )
        for durable in ("demo-player", "demo-receiver"):
            try:
                await js.delete_consumer(stream_name, durable)
            except NotFoundError:
                continue

    async def run_async() -> None:
        cluster = JetStreamClusterManager(log_dir=args.log_dir)
        datastore_cluster: TimescaleHACluster | None = None
        archiver_task: asyncio.Task[None] | None = None
        generator_proc: subprocess.Popen[str] | None = None
        command_proc: subprocess.Popen[str] | None = None
        player_proc: subprocess.Popen[str] | None = None
        stop_event = threading.Event()
        shutdown_event = asyncio.Event()
        try:
            cluster.start()
            datastore_cluster = TimescaleHACluster(replicas=2)
            await datastore_cluster.connect()
            replica_list = ", ".join(datastore_cluster.replica_names())
            print(
                f"Started HA datastore with {datastore_cluster.replica_count} nodes ({replica_list})."
            )
            loop = asyncio.get_running_loop()
            nc: NATS | None = None
            try:
                nc = await connect_to_cluster(cluster.client_urls)
                js = nc.jetstream()
                await prepare_stream(js, cluster.replicas)

                if datastore_cluster is None:
                    raise RuntimeError("Datastore cluster failed to initialise")

                archiver = Archiver(js, datastore_cluster, durable_prefix="demo")

                async def archiver_worker() -> None:
                    try:
                        while not stop_event.is_set():
                            try:
                                stored = await archiver.drain()
                            except Exception as exc:  # pragma: no cover - diagnostics
                                print(f"[datastore] archiver error: {exc}", file=sys.stderr)
                                await asyncio.sleep(1.0)
                                continue
                            if stored:
                                total = await datastore_cluster.count_messages()
                                replica_totals = await datastore_cluster.replica_message_counts()
                                replica_status = ", ".join(
                                    f"{name}={count}"
                                    for name, count in zip(
                                        datastore_cluster.replica_names(), replica_totals
                                    )
                                )
                                print(
                                    f"[datastore] persisted {stored} message(s) (total {total}; {replica_status})",
                                    flush=True,
                                )
                            else:
                                await asyncio.sleep(0.5)
                    except asyncio.CancelledError:  # pragma: no cover - cancellation handling
                        raise

                archiver_task = asyncio.create_task(archiver_worker())

                qt_env = os.environ.copy()
                qt_env.pop("QT_QPA_PLATFORM", None)

                generator_cmd: List[str] = [
                    sys.executable,
                    str(PROJECT_ROOT / "tspi_generator_flet.py"),
                    "--count",
                    str(args.count),
                    "--rate",
                    str(args.rate),
                    "--duration",
                    "1.0",
                    "--continuous",
                    "--js-stream",
                    "TSPI",
                    "--stream-prefix",
                    "tspi",
                ]
                for url in cluster.client_urls:
                    generator_cmd.extend(["--nats-server", url])

                player_cmd: List[str] = [
                    sys.executable,
                    str(PROJECT_ROOT / "player_flet.py"),
                    "--metrics-interval",
                    "1.0",
                    "--room",
                    "demo",
                    "--durable-prefix",
                    "demo",
                    "--js-stream",
                    "TSPI",
                    "--source",
                    "live",
                ]
                for url in cluster.client_urls:
                    player_cmd.extend(["--nats-server", url])

                command_cmd: List[str] = [
                    sys.executable,
                    str(PROJECT_ROOT / "command_console_flet.py"),
                    "--sender-id",
                    "demo-ui",
                    "--js-stream",
                    "TSPI",
                    "--ops-stream",
                    "TSPI_OPS",
                    "--stream-prefix",
                    "tspi",
                ]
                for url in cluster.client_urls:
                    command_cmd.extend(["--nats-server", url])

                try:
                    generator_proc = subprocess.Popen(generator_cmd, env=qt_env)
                    print(f"Launched generator UI: {shlex.join(generator_cmd)}", flush=True)
                except Exception as exc:
                    raise RuntimeError(f"Failed to launch generator UI: {exc}") from exc

                try:
                    player_proc = subprocess.Popen(player_cmd, env=qt_env)
                    print(f"Launched receiver UI: {shlex.join(player_cmd)}", flush=True)
                except Exception as exc:
                    if generator_proc is not None and generator_proc.poll() is None:
                        generator_proc.terminate()
                    if command_proc is not None and command_proc.poll() is None:
                        command_proc.terminate()
                    raise RuntimeError(f"Failed to launch receiver UI: {exc}") from exc

                try:
                    command_proc = subprocess.Popen(command_cmd, env=qt_env)
                    print(f"Launched command UI: {shlex.join(command_cmd)}", flush=True)
                except Exception as exc:
                    for proc in (player_proc, generator_proc):
                        if proc is not None and proc.poll() is None:
                            proc.terminate()
                    raise RuntimeError(f"Failed to launch command UI: {exc}") from exc

                async def monitor_process(name: str, proc: subprocess.Popen[str]) -> None:
                    await asyncio.to_thread(proc.wait)
                    if not shutdown_event.is_set():
                        print(f"{name} exited; shutting down demo.", flush=True)
                        request_shutdown()

                loop.create_task(monitor_process("Generator UI", generator_proc))
                loop.create_task(monitor_process("Receiver UI", player_proc))
                loop.create_task(monitor_process("Command UI", command_proc))

                def request_shutdown() -> None:
                    if not shutdown_event.is_set():
                        print("Shutdown requested. Cleaning up...", flush=True)
                        stop_event.set()
                        shutdown_event.set()

                for sig in (signal.SIGINT, signal.SIGTERM):
                    try:
                        loop.add_signal_handler(sig, request_shutdown)
                    except NotImplementedError:  # pragma: no cover - fallback for non-UNIX
                        signal.signal(sig, lambda _sig, _frame: request_shutdown())

                if args.duration is not None:
                    async def auto_stop() -> None:
                        await asyncio.sleep(max(0.1, args.duration))
                        request_shutdown()

                    loop.create_task(auto_stop())

                print("Demo environment is running. Press Ctrl+C to stop.")
                await shutdown_event.wait()
            finally:
                stop_event.set()
                for name, proc in (
                    ("generator UI", generator_proc),
                    ("receiver UI", player_proc),
                    ("command UI", command_proc),
                ):
                    if proc is None:
                        continue
                    if proc.poll() is None:
                        proc.terminate()
                        try:
                            proc.wait(timeout=10)
                        except subprocess.TimeoutExpired:
                            proc.kill()
                            try:
                                proc.wait(timeout=5)
                            except subprocess.TimeoutExpired:
                                pass
                    exit_code = proc.returncode
                    print(f"{name} exited with code {exit_code}.", flush=True)
                if archiver_task is not None:
                    archiver_task.cancel()
                    try:
                        await archiver_task
                    except asyncio.CancelledError:  # pragma: no cover - expected on shutdown
                        pass
                if datastore_cluster is not None:
                    total_messages = await datastore_cluster.count_messages()
                    replica_totals = await datastore_cluster.replica_message_counts()
                    replica_status = ", ".join(
                        f"{name}={count}"
                        for name, count in zip(datastore_cluster.replica_names(), replica_totals)
                    )
                    total_commands = await datastore_cluster.count_commands()
                    total_tags = await datastore_cluster.count_tags()
                    print(
                        "Datastore summary: "
                        f"{total_messages} message(s), {total_commands} command(s), {total_tags} tag(s) "
                        f"across replicas ({replica_status}).",
                        flush=True,
                    )
                    await datastore_cluster.close()
                if nc is not None:
                    try:
                        await nc.drain()
                    finally:
                        await nc.close()
        finally:
            cluster.stop()

    try:
        asyncio.run(run_async())
    except KeyboardInterrupt:
        return 1
    except Exception as exc:
        print(f"Demo failed: {exc}", file=sys.stderr)
        return 1
    return 0


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
